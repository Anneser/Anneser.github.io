[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "research",
    "section": "",
    "text": "Coming soon…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Github\n  \n  \n    \n     Linkedin\n  \n  \n    \n     ORCID\n  \n\n      \nHi, welcome to my page! I am postdoc in the field of Systems Neuroscience and I am interested in state-dependent information processing in neural circuits. These days, I work in the group of Rainer Friedrich at the Friedrich Miescher Institute for Biomedical Research in Basel, Switzerland. Before that, I did my PhD with Erin Schuman at the Max Planck Institute for Brain Research in Frankfurt, Germany. On this site, I give an overview of previous projects and current ideas. Feel free to look around!\nMy journey in science started in 2010, when I enrolled at the University of Erlangen-Nuremberg in the north of Bavaria. I majored in Molecular Medicine, and joined the lab of Beate Winner for my thesis work. Shinya Yamanaka had just won the Nobel prize for his groundbreaking work on the induction of pluripotent stem cells. In a nutshell, these insights enabled researchers to retrieve cells from human patients, re-program them to the pluripotent stage and differentiate them into cell types of interest, such as neural cells. That made it possible to work on actual human neuron-like cells in a lab to better understand diseases which were hard to model in other systems at the time. The Winner lab did exactly that, focusing on neurodegenerative diseases such as ALS or HSP. During my time there, for the first time, I was exposed to molecular biology techniques and actual lab work - a fascinating world that I would continue to explore for the years to come. My work there resulted in a co-authorship on a paper on SPG11, one of the genes responsible for the onset of HSP.\nI decided to fully focus on neuroscience and moved to Heidelberg for my graduate studies. Here, I wanted to get some broad exposure to different aspects of the brain and applied for internships in a variety of labs. I initially did some rotations in groups performing electrophysiological recordings in rodents ex vivo and in vivo and later went for half a year to New Haven to work in the Colón-Ramos lab on synaptogenesis in C. elegans. I did my thesis work with Detlev Arendt at the EMBL on the topic of cell type evolution in the annelid Platynereis dumerilii. After getting some impression of what kind of questions I felt most drawn to, I applied for the International Max Planck Research School (IMPRS) centered around the MPI for Brain Research in Frankfurt.\nDuring my time in the group of Erin Schuman, I tried to understand how social isolation impacts the brain transcriptome. This work resulted in some interesting insights into the role of the neuropeptide Pth2 in responding to different social environments and regulating appropriate behavioral responses. I also collaborated with a fellow PhD student on the behavioral analysis of oxytocin-receptor deficient fish. By and large, I developed quite some expertise on the level of molecular biology techniques and behavioral analysis.\nFor my postdoc, I joined the lab of Rainer Friedrich at the FMI in Basel, Switzerland. This lab excels at the systematic analysis of neural circuit connectivity and physiological recordings of neuronal populations using 2-photon imaging. I was keen on expanding my technical repertoire to systems level approaches such as these, but I first wanted to leverage my already established expertise and conducted a single-cell sequencing screen of the zebrafish telencephalon to identify cell populations warranting a closer look. This analysis suggested that the activity of neurons (i.e., the way they process information) is subject to modulation by a plethora of so-called neuromodulators, small molecules secreted by other cells in the brain that have lasting consequences on the physiological parameters determining neuronal activity. For the time being, I am very keen to understand how this combinatorial complexity of neuromodulatory actions could be understood."
  },
  {
    "objectID": "blog/polars_vs_pandas/index.html",
    "href": "blog/polars_vs_pandas/index.html",
    "title": "Cute Pandas and Fast Polars",
    "section": "",
    "text": "Data analysis - the cookbook\nIf you analyse data using python, most likely you have developed some good out-of-the box routines you can apply to most of your datasets. Probably you start with a manual inspection of the data types, settle on a suitable approach to read in files, follow along with some sanity checks and solid data visualization to get a feeling for the data at hand. Most likely, though, you start by typing the following:\n\nimport numpy as np\nimport pandas as pd\n\nBoth NumPy and Pandas reliably make it to the top of the list of popular python libraries. NumPy (short for numerical python) comes with a lot of inbuilt capabilities for handling arrays, making it a go-to library for scientific computing and machine learning. Pandas uses these powerful array-manipulation capabitilies and introduces Series (1D) and DataFrames (2D) and allows the user to rather easily slice through the dataset in different ways, compute central tendencies or other statistical descriptors for individual columns and rows. Additionally, it quite naturally can be connected to downstream libraries such as seaborn for data visualization.\nBasically all my pipelines at some point use pandas (and I am not alone with that), so it would be a huge leverage factor to speed up the storage and retrieval process regarding my data structures. Introducing: Polars.\n\n\nCute Pandas, fast Polars\nI happened to come across this library a little while ago and was intrigued by their promise: similar capabilites as Pandas, but at a fraction of the time needed for execution. The edge comes from two major factors: First, the library is written in Rust, known to rival C, in particular when it comes to parallelization. Second, it introduces lazy evaluation (computations are only executed when needed) and query optimization (we will come to that). On their website, [pola.rs][(https://pola.rs/)] (I do find it funny…), they claim a time advantage over pandas for a common data wrangling operation of roughly 90-fold. If that is not enough to get you interested, you never had to run you pipeline over night (or several days) and reading further might be a genuine waste of time for you. Anyway, I wanted to see whether I get similar results. Some of the most typical operations you want to apply to your data concern I/O (input/output, so the reading and writing of files), computation of some sort with the data loaded in memory and the transformation of the dataset (e.g., to tidy format). I created three datasets with random numbers across three columns and 100.000, 1.000.000, or 5.000.000 rows. Obviously, there could be bigger datasets, but I daresay this should cover typical use cases. If you want to re-create this for yourself, here is the code for the random datasets:\n\nimport time, gc, os, statistics as stats\nimport numpy as np\nimport pandas as pd\n\n# polars can be easily installed via pip install polars\nimport polars as pl\n\ndef make_sales(n_rows: int, out_file: str, seed: int = 42):\n    rng = np.random.default_rng(seed)\n    store_ids = rng.integers(1, 5000, size=n_rows, dtype=np.int32)\n    revenue = rng.lognormal(mean=5, sigma=1.0, size=n_rows).round(2)\n    dates = pd.to_datetime(\"2020-01-01\") + pd.to_timedelta(\n        rng.integers(0, 365, size=n_rows), unit=\"D\"\n    )\n    df = pd.DataFrame({\"store_id\": store_ids, \"revenue\": revenue, \"date\": dates})\n    df.to_csv(out_file, index=False)\n    return out_file\npaths = [\n    (\"sales_100k.csv\", 100_000),\n    (\"sales_1M.csv\",   1_000_000),\n    (\"sales_5M.csv\",   5_000_000),\n]\n\ngenerated = []\nfor fname, n in paths:\n    if not os.path.exists(fname):\n        make_sales(n, fname)\n    generated.append((fname, n))\ngenerated\n\n[('sales_100k.csv', 100000),\n ('sales_1M.csv', 1000000),\n ('sales_5M.csv', 5000000)]\n\n\nThe cell above will create the three example datasets in the directory from which you run the script. I admit I was not very creative (sales.csv is a pretty common toy dataset), but feel free to adapt with columns and data types more suitable for your use cases. The next step was to create a quick function to time the execution of the typical data wrangling procedures:\n\ndef timer_samples(fn, repeat=5):\n    import gc, time, statistics as stats\n    samples = []\n    for _ in range(repeat):\n        gc.collect()\n        t0 = time.perf_counter()\n        fn()\n        samples.append(time.perf_counter() - t0)\n    return {\n        \"samples_s\": samples,\n        \"min_s\": min(samples),\n        \"median_s\": stats.median(samples),\n        \"mean_s\": sum(samples)/len(samples),\n        \"std_s\": (stats.pstdev(samples) if len(samples) &gt; 1 else 0.0),\n    }\n\n\ndef fmt_ms(sec): \n    return round(sec * 1000, 1)\n\nThe timer function executes a function several times and returns some parameters describing the execution speed. With this, we are now ready to get into polars vs pandas specifics.\n\n\nInput/Output\nUsing Pandas, you load datasets typically with a built-in method such as pd.read_csv(). In Polars, the same operation comes in two flavors: Either you directly read in the dataset (eager mode) via pl.read_csv() (Polars was written with familiarity to Pandas in mind) or you use lazy mode. Here, you actually don’t directly load the dataset, but build a query plan via pl.scan_csv(). The plan can then be materialized later using the collect() command. If you immediately combine scanning and collecting, there is no advantage whatsoever, so pl.scan_csv(path).collect() does not make any sense. It shines when you combine the scanning with data wrangling procedures and collect in the end because here query optimization can make a huge difference. We get to this later. One interesting aspect about Polars is its way to parse datatypes. In Pandas, you would read in a .csv like this:\npd.read_csv(path, dtype={\"integer_type\": \"int32\"})\nIn Polars (eager mode), you do the following:\npl.read_csv(path, dtypes={\"integer_type\": pl.Int32})\nPassing datatypes to I/O functions is good practice for a couple of reasons: You avoid surprises later on when you try to combine the values from two columns that have been assigned incompatible datatypes by under-the-hood inference of your I/O function. It also allows for memory-aware processing (Int32 allocates half the memory of Int64) which definitely comes in handy for large datasets. So, why does Polars assign the pl.Int32 type? Because it is NOT based on NumPy! Instead, it uses Apache Arrows columnar memory with its own datatypes that map cleanly to Arrow and allows for query optimization.\n\n\nFiltering Operations\nA typical step for any data analysis is filtering. Interested in particularly huge values? Filter. Care about cases within one standard deviation of the mean? Filter. Want to exclude outliers? Filter. Of course, both Pandas and Polars come with built-in capabilities for this. In Pandas, you can identify cases of interest by selecting a column and performing logical indexing:\ndf.loc[df[\"column1\"] &gt; 1000, [\"column1\", \"column2\", \"column4\"]]\nThis command selects all rows of column 1 with a value over 1000 and returns the selected rows for columns 1, 2, and 4. In Polars, you get the same with\ndf.lazy()\n    .filter(pl.col(\"column1\") &gt; 1000)\n    .select([\"column1\", \"column2\", \"column4\"])\n    .collect()\nWhen you go for lazy mode, you might actually prefer to start with pl.scan_csv() to fully leverage Polars. For comparability, I assumed here that the dataset is already im memory.\n\n\nGrouping\nOften, we are interested in computing average values for certain groups within our data or compare the number of entries per category. This can be achieved by grouping operations and consecutive aggregation across the resulting groups. Here is an example for Pandas:\ndf.groupby(\"column1\", as_index=False).agg(\n            column2_sum=(\"column2\", \"sum\"),\n            n=(\"column2\", \"size\"),\n            avg=(\"column2\", \"mean\"))\nWe group across the different values of column 1 and compute the sum, number of entries, and average values for column 2 for all individual values of column 1. Here is the same approach in Polars:\ndf.lazy()\n    .group_by(\"column1\")\n    .agg([\n        pl.col(\"column2\").sum().alias(\"column2_sum\"),\n        pl.count().alias(\"n\"),\n        pl.col(\"column2\").mean().alias(\"column2_avg\"),\n        ])\n    .collect()\nOne neat feature of Polars is the .alias() function which allows you to compute values and assign the column name directly. As you can see, the the commands can easily be stacked and due to native multi-core usage, this allows for speedy processing. If you wanted to fully leverage Polars, you start by scanning the file, listing all operations you want to execute, leave it to polars to optimize the query plan and collect in the end.\n\n\nMerging and Tidying up\nA last common approach is data reshaping. Many kinds of analysis require the dataset to exist in a certain format and not always is a simple transpose the answer. I implemented a simple merging and melting operation for both Pandas and Polars, now let us see how execution speed differs for the tasks outlined above. In the benchmarking, I differentiate between I/O and computation on loaded datasets but keep in mind that Polars really shines by combining scanning, computation, and collection.\n\n\nExpand cell\ndef bench_one_dataset_sampled(path: str, repeat: int = 7):\n    import pandas as pd\n    import polars as pl\n    import os\n\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"{path} not found\")\n\n    # --- IO variants\n    def pandas_read():\n        return pd.read_csv(path, dtype={\"store_id\": \"int32\"}, parse_dates=[\"date\"])\n    def polars_read():\n        return pl.read_csv(path, dtypes={\"store_id\": pl.Int32, \"date\": pl.Date})\n    def polars_scan_collect():\n        return pl.scan_csv(path, dtypes={\"store_id\": pl.Int32, \"date\": pl.Date}).collect()\n\n    io_variants = [\n        (\"IO\", \"pandas read_csv\", pandas_read),\n        (\"IO\", \"polars read_csv\", polars_read),\n        (\"IO\", \"polars scan-&gt;collect\", polars_scan_collect),\n    ]\n\n    io_rows, io_samples = [], []\n    for phase, variant, fn in io_variants:\n        res = timer_samples(fn, repeat=repeat)\n        io_rows.append({\n            \"dataset\": path, \"phase\": phase, \"variant\": variant,\n            \"min_ms\": fmt_ms(res[\"min_s\"]),\n            \"median_ms\": fmt_ms(res[\"median_s\"]),\n            \"mean_ms\": fmt_ms(res[\"mean_s\"]),\n            \"std_ms\": fmt_ms(res[\"std_s\"]),\n            \"n\": repeat,\n        })\n        for i, s in enumerate(res[\"samples_s\"]):\n            io_samples.append({\n                \"dataset\": path, \"phase\": phase, \"variant\": variant,\n                \"run\": i+1, \"ms\": fmt_ms(s)\n            })\n\n    # Load once for compute timings\n    pdf  = pandas_read()\n    pldf = polars_read()\n\n    # tiny dim table\n    max_store = int(pdf[\"store_id\"].max())\n    dim_pd = pd.DataFrame({\n        \"store_id\": pd.Series(range(1, max_store + 1), dtype=\"int32\"),\n        \"region\": (pd.Series(range(1, max_store + 1)) % 20).astype(\"int8\"),\n    })\n    dim_pl = pl.DataFrame(dim_pd)\n\n    # --- Tasks \n    def pandas_task1():\n        out = (pdf.loc[pdf[\"revenue\"] &gt; 1000, [\"store_id\", \"revenue\", \"date\"]]\n                  .assign(rev_k=lambda d: d[\"revenue\"]/1000.0))\n        return float(out[\"rev_k\"].sum())\n\n    def polars_task1_eager():\n        out = (pldf.filter(pl.col(\"revenue\") &gt; 1000)\n                    .select([\"store_id\", \"revenue\", \"date\"])\n                    .with_columns((pl.col(\"revenue\")/1000.0).alias(\"rev_k\")))\n        return float(out[\"rev_k\"].sum())\n\n    def polars_task1_lazy():\n        out = (pldf.lazy()\n                   .filter(pl.col(\"revenue\") &gt; 1000)\n                   .select([\"store_id\", \"revenue\", \"date\"])\n                   .with_columns((pl.col(\"revenue\")/1000.0).alias(\"rev_k\"))\n                   .collect())\n        return float(out[\"rev_k\"].sum())\n\n    def pandas_task2():\n        g = pdf.groupby(\"store_id\", as_index=False).agg(\n            revenue_sum=(\"revenue\", \"sum\"),\n            n=(\"revenue\", \"size\"),\n            avg=(\"revenue\", \"mean\"),\n        )\n        share = pdf[\"revenue\"] / pdf[\"revenue\"].sum()\n        return float(g[\"revenue_sum\"].sum() + share.sum())\n\n    def polars_task2_eager():\n        g = (pldf.group_by(\"store_id\")\n                 .agg([\n                     pl.col(\"revenue\").sum().alias(\"revenue_sum\"),\n                     pl.count().alias(\"n\"),\n                     pl.col(\"revenue\").mean().alias(\"avg\"),\n                 ]))\n        df2 = pldf.with_columns((pl.col(\"revenue\")/pl.col(\"revenue\").sum()).alias(\"share\"))\n        return float(g[\"revenue_sum\"].sum() + df2[\"share\"].sum())\n\n    def polars_task2_lazy():\n        g = (pldf.lazy()\n                 .group_by(\"store_id\")\n                 .agg([\n                     pl.col(\"revenue\").sum().alias(\"revenue_sum\"),\n                     pl.count().alias(\"n\"),\n                     pl.col(\"revenue\").mean().alias(\"avg\"),\n                 ])\n                 .collect())\n        df2 = (pldf.lazy()\n                    .with_columns((pl.col(\"revenue\")/pl.col(\"revenue\").sum()).alias(\"share\"))\n                    .collect())\n        return float(g[\"revenue_sum\"].sum() + df2[\"share\"].sum())\n\n    def pandas_task3():\n        merged = pdf.merge(dim_pd, on=\"store_id\", how=\"left\")\n        tidy = merged.melt(id_vars=[\"store_id\", \"date\", \"region\"],\n                           value_vars=[\"revenue\"],\n                           var_name=\"metric\", value_name=\"value\")\n        return len(tidy)\n\n    def polars_task3_eager():\n        merged = pldf.join(dim_pl, on=\"store_id\", how=\"left\")\n        tidy = merged.melt(id_vars=[\"store_id\", \"date\", \"region\"],\n                           value_vars=[\"revenue\"],\n                           variable_name=\"metric\", value_name=\"value\")\n        return tidy.height\n\n    def polars_task3_lazy():\n        out = (pldf.lazy()\n                   .join(dim_pl.lazy(), on=\"store_id\", how=\"left\")\n                   .melt(id_vars=[\"store_id\", \"date\", \"region\"],\n                         value_vars=[\"revenue\"],\n                         variable_name=\"metric\", value_name=\"value\")\n                   .collect())\n        return out.height\n\n    compute_variants = [\n        (\"Filter/Select/Transform\", \"pandas\",        pandas_task1),\n        (\"Filter/Select/Transform\", \"polars eager\",  polars_task1_eager),\n        (\"Filter/Select/Transform\", \"polars lazy\",   polars_task1_lazy),\n        (\"GroupBy & Window\",        \"pandas\",        pandas_task2),\n        (\"GroupBy & Window\",        \"polars eager\",  polars_task2_eager),\n        (\"GroupBy & Window\",        \"polars lazy\",   polars_task2_lazy),\n        (\"Join & Melt\",             \"pandas\",        pandas_task3),\n        (\"Join & Melt\",             \"polars eager\",  polars_task3_eager),\n        (\"Join & Melt\",             \"polars lazy\",   polars_task3_lazy),\n    ]\n\n    comp_rows, comp_samples = [], []\n    for phase, variant, fn in compute_variants:\n        res = timer_samples(fn, repeat=repeat)\n        comp_rows.append({\n            \"dataset\": path, \"phase\": phase, \"variant\": variant,\n            \"min_ms\": fmt_ms(res[\"min_s\"]),\n            \"median_ms\": fmt_ms(res[\"median_s\"]),\n            \"mean_ms\": fmt_ms(res[\"mean_s\"]),\n            \"std_ms\": fmt_ms(res[\"std_s\"]),\n            \"n\": repeat,\n        })\n        for i, s in enumerate(res[\"samples_s\"]):\n            comp_samples.append({\n                \"dataset\": path, \"phase\": phase, \"variant\": variant,\n                \"run\": i+1, \"ms\": fmt_ms(s)\n            })\n\n    import pandas as pd\n    summary_df = pd.DataFrame(io_rows + comp_rows)\n    samples_df = pd.DataFrame(io_samples + comp_samples)\n    return summary_df, samples_df\n\n# Run the sampled benchmark for your three CSVs:\nsummary_list, samples_list = [], []\nfor fname, _ in generated:\n    s, smp = bench_one_dataset_sampled(fname, repeat=7)  \n    summary_list.append(s)\n    samples_list.append(smp)\n\nsummary_df = pd.concat(summary_list, ignore_index=True)\nsamples_df = pd.concat(samples_list, ignore_index=True)\n\n# Take a quick look\nsummary_df.head(), samples_df.head()\n\n\n(          dataset                    phase               variant  min_ms  \\\n 0  sales_100k.csv                       IO       pandas read_csv    25.5   \n 1  sales_100k.csv                       IO       polars read_csv     1.9   \n 2  sales_100k.csv                       IO  polars scan-&gt;collect     2.9   \n 3  sales_100k.csv  Filter/Select/Transform                pandas     1.1   \n 4  sales_100k.csv  Filter/Select/Transform          polars eager     3.5   \n \n    median_ms  mean_ms  std_ms  n  \n 0       26.2     26.5     0.9  7  \n 1        2.0      2.3     0.7  7  \n 2        3.1      3.4     0.7  7  \n 3        1.2      1.3     0.2  7  \n 4        3.6      3.7     0.3  7  ,\n           dataset phase          variant  run    ms\n 0  sales_100k.csv    IO  pandas read_csv    1  27.9\n 1  sales_100k.csv    IO  pandas read_csv    2  26.0\n 2  sales_100k.csv    IO  pandas read_csv    3  25.9\n 3  sales_100k.csv    IO  pandas read_csv    4  28.0\n 4  sales_100k.csv    IO  pandas read_csv    5  25.5)\n\n\n\n\nComparison of Pandas and Polars for common data wrangling procedures\nAlright, let’s visualize the results:\n\n\nExpand cell\n# Grouped bars: one figure per phase, three datasets in one plot\nimport re\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef _parse_size_from_name(ds: str):\n    \"\"\"Return (numeric_rows_for_sort, short_label) from a filename like 'sales_1M.csv'.\"\"\"\n    stem = Path(ds).stem\n    m = re.search(r'(\\d+)\\s*([kKmM]?)', stem)\n    if not m:\n        return (0, stem)\n    val = int(m.group(1))\n    suf = m.group(2).lower()\n    mult = 1_000 if suf == \"k\" else (1_000_000 if suf == \"m\" else 1)\n    return (val * mult, f\"{val}{suf.upper()}\" if suf else str(val))\n\ndef plot_phase_grouped(summary: pd.DataFrame, phase: str, title_suffix: str = \"\", save=False):\n    # Select phase and keep only the variants we care about, in a fixed order\n    if phase == \"IO\":\n        order = [\"pandas read_csv\", \"polars read_csv\", \"polars scan-&gt;collect\"]\n    else:\n        order = [\"pandas\", \"polars eager\", \"polars lazy\"]\n\n    df = summary[summary[\"phase\"] == phase].copy()\n    if df.empty:\n        print(f\"(no rows for phase: {phase})\")\n        return\n\n    # Datasets: sort by numeric size (100k &lt; 1M &lt; 5M, etc.)\n    datasets = sorted(df[\"dataset\"].unique(), key=lambda s: _parse_size_from_name(s)[0])\n    ds_labels = [_parse_size_from_name(s)[1] for s in datasets]\n\n    # X locations (one group per variant)\n    x = np.arange(len(order))\n    width = 0.25  # width of each bar\n\n    plt.figure(figsize=(9, 5))\n    for i, ds in enumerate(datasets):\n        d = df[df[\"dataset\"] == ds].copy()\n        d[\"variant\"] = pd.Categorical(d[\"variant\"], categories=order, ordered=True)\n        d = d.sort_values(\"variant\")\n\n        heights = d[\"median_ms\"].astype(float).values\n        yerr    = d[\"std_ms\"].astype(float).values\n\n        # shift each dataset’s bars sideways\n        plt.bar(x + (i - (len(datasets)-1)/2) * width, heights, width, yerr=yerr, capsize=6, label=ds_labels[i])\n\n    plt.xticks(x, order, rotation=15, ha=\"right\")\n    plt.ylabel(\"Time (ms, median ± std)\")\n    title = f\"{phase}\"\n    if title_suffix:\n        title += f\" — {title_suffix}\"\n    plt.title(title)\n    plt.legend(title=\"Rows\")\n    plt.tight_layout()\n\n    if save:\n        out = f\"plot_grouped_{phase.replace(' ', '_').lower()}.png\"\n        plt.savefig(out, dpi=160)\n    plt.show()\n\n# Usage: one figure per phase, each shows 100k/1M/5M together\nplot_phase_grouped(summary_df, \"IO\", title_suffix=\"CSV read\")\nplot_phase_grouped(summary_df, \"Filtering and Selecting\")\nplot_phase_grouped(summary_df, \"GroupBy\")\nplot_phase_grouped(summary_df, \"Merging & Melting\")\n\n\n\n\n\n\n\n\n\n(no rows for phase: Filtering and Selecting)\n(no rows for phase: GroupBy)\n(no rows for phase: Merging & Melting)\n\n\nAs you can see, Polars does indeed deliver. If your datasets are not too big, it should not matter too much, but once you enter millions of rows or have to load dozens of datasets, Polars really shines. Fair enough, for the I/O section, it did not make too much sense to compare scan&gt;collect with read_csv() (for reasons outlined above), but if you fully commit and combine scanning with the other data wrangling procedure described here, you will get the benefit. Even though, loading times alone are probably making a good case for trying Polars.\n\n\nPros and Cons\nAlright, this concludes the comparison. If you have large datasets, Polars makes a lot of sense for you to try. However, keep in mind that it comes at a cost: Pandas is still the predominant library for data manipulation and has a huge ecosystem. Chances are that questions regarding how to do things in Polars might be more difficult to answer than the same questions for Pandas. Another thing is downstream applicability. A lot of libraries are fine with Pandas.Series or DataFrames as input structures, but not many accept a Polars dataframe. You can in principle export Polars dataframes as Pandas structures, but that seems to be an ugly work-around."
  },
  {
    "objectID": "blog/20250706/index.html",
    "href": "blog/20250706/index.html",
    "title": "False friends in biology",
    "section": "",
    "text": "Thought follows language\nIf you have ever tried to learn a new language, you might have encountered false friends. These words sound awfully familiar to a word in your native tongue but their meaning is completely different. Many a german has stumbled over the english word become, which seems kinda close to the german bekommen (to get), leading to the infamous joke of the german asking the waiter when he might become his schnitzel (reply: “I hope never, sir”).\nIn a way, it is only natural that we try to map the meaning of words to the semantic information they evoke (which might be distinct from the semantic information they actually carry). In many cases, this might be used to progressively adapt our language to better delineate what we actually want to say - not much sense in using words that our conversation partners will associate with concepts different from what we actually want to discuss.\nAll the more one would suspect this to hold true in the context of highly specialized expert languages. As new concepts are worked out, special terms are invented and defined to precisely convey the technicalities inherent to the conceptual space. However, for reasons to be discussed, this does not hold true for biology.\n\n\nLanguage leads astray\nStudying biology is wild. On many fronts, empiricism outpaces theory by a far margin and it is not at all apparent when this trend might change. That means that whenever a biologist finds something new - be it a genetic sequence, a protein, a cell type or some previously unknown process - they have no idea about what its characteristic features are and what it might be good for in a wider context. In many cases, whatever appeared to be a relevant characteristic inspired nomenclature.\nThis led to some remarkable naming decisions. Wanna take a guess what the function of the prolactin-releasing peptide (PrRP) might be? As a matter of fact, it appears to do quite many things, but the release of prolactin is a secondary effect at best. In 1998, scientists specifically set out to find a molecule triggering the release of prolactin and found PrRP to be quite efficient in vitro. However, rodents models with non-functional PrRP or its cognate receptor have quite normal prolactin levels. They are, however, obese, indicating a role in metabolism rather than the control of prolactin release.\nWhat about another example? Where do you think parathyroid hormone 2 (Pth2) is produced? Only a fool would be believe it to be the parathyroid gland. Obviously, the cells expressing it are found either in the thalamus or, depending on the species, also the medulla-pons junction. But at the very least it is a hormone, correct? Alas, as far as we know, it does not enter the bloodstream, so it does not qualify as such. This particular protein was identified in the 90s by Ted Usdin, who, back then, named it tuberoinfundibular peptide of 39 amino acids length (TIP39). It was identified as the endogenous ligand binding to a curious receptor found in the brain whose sequence was quite similar to that of the parathyroid hormone receptor. It was thus later re-named to convey that similarity. Take note of how that decision was made - one system was already described and well-known, so the one to be discovered later was simply baptized accordingly and a number attached.\nAnother fun story is the one of parvalbumin-expressing interneurons. If you are interested in neuroscience and cortical circuits on any level, you probably have heard of the classical marker genes distinguishing the interneurons found in mammalian circuits:\n\nPV (parvalbumin)\nCCK (cholecystokinin)\nVIP (vasoactive intestinal peptide)\nSST (somatostatin)\n\nTo find these kind of markers was an amazing step forward because it helped to analyze whether different kinds of interneuron displayed particular connectivity patterns or responded only in certain situations. However, referring to interneurons by the marker they express (it is quite common to say something like “SST interneurons”) might also lead to the assumption that a PV interneuron in a a chimpanzee fullfills the same function than in a rodent. It is a good hypothesis, but there are lots of idiosyncracies in different species and assumptions like these have to be carefully validated. Conversely, one might look for PV interneurons in lizards and not find any, concluding that this cell type is missing and only evolved later. In reality, PV interneurons exist in lizards and simply do not express PV. What a bummer, when the gene the cell type is named after is simply not expressed.\n\n\nA map needs to reflect the territory\nIt would be easy to extend the list of examples, but I think I made my point. Biology is at a crossroads, where technical terms become unnecessarily opaque and the meaning evoked by words is not at all the meaning they carry. In specialist communities, one can get away with this for a certain period of time, but the very point of having technical terms is to make a specific concept extremely clear. Language is a kind of map that helps us navigate complex conceptual territories. If you encounter a map which uses the tree symbol to show roads and reverses the usual colors of land and sea, it will rather confuse you than help you get from A to B. In particular if the community making these maps doesn’t even think it necessary to explain the new standard. Obviously, the map is not the territory, but it should reflect it as well as it can to be useful.\nI do not have a simple solution to the widespread use of misnomers in biology. Mostly, they came into being as completely sound and rational naming decisions and later, they were not changed as to not pollute their trail through the scientific literature (it is quite the inconvenience to a proper literature search if, for example, genes of interest have been renamed repeatedly). However, as we go on, we should be careful not to muddle our thinking by using opaque words but rather seek clarity in both thought and language."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog/20250517/index.html",
    "href": "blog/20250517/index.html",
    "title": "Bridging levels - the importance of cell type identity",
    "section": "",
    "text": "Over recent decades, one major theme of neuroscience was a switch from the analysis of individual neurons to recording ever-larger numbers of cells and trying to understand their joint behavior - the population activity. In many ways, this development was unavoidable and the result of a simple numbers game: while some invertebrate species only have a small number of highly stereotyped neurons (in the nematode C. elegans, all 302 neurons are even uniquely named and can be identified across animals), most animals have far over 100 000 neurons. As one enters the class of mammals, that number can safely be assumed to be in the millions.\nIn order to better understand what the concurrent activity in all these cells mean, it is not sufficient to simply increase the number of neurons one records from. In the end, one is left with a gigantic matrix to make sense of and without a way to simplify, that is a rather daunting task . One important insight was that in a population of neurons, the activity of some subpopulations tends to be correlated. Depending on the strength of correlations, dimensionality reduction techniques such as principal component analysis can be applied, allowing for a lower-dimensional representation of the dataset without sacrificing too much variability. Treating neuronal population recordings this way allowed for fascinating discoveries, showcasing how the topology of the subspace carved out by neuronal dynamics is intricately linked to the function of the circuit.\nOf course, there is no free lunch and data processing can only ever destroy information. What is sacrificed in this context is knowledge about individual cells and their responses. Now, one might assume that the trade-off is worth it, but progress in biology is dependent on the combination of insights on different levels of understanding. I recently came across a really interesting paper from the lab of Liset Menendez de la Prida, that is beautifully illustrating this point. The publication is called Cell-type-specific manifold analysis discloses independent geometric transformations in the hippocampal spatial code and is quite a read. Julio Esparza and colleagues try to understand the role of genetically defined subpopulations of neurons in the hippocampal subregion CA1 in the formation of spatial representations."
  },
  {
    "objectID": "blog/20250517/index.html#from-single-neuron-recordings-to-population-level-analysis",
    "href": "blog/20250517/index.html#from-single-neuron-recordings-to-population-level-analysis",
    "title": "Bridging levels - the importance of cell type identity",
    "section": "",
    "text": "Over recent decades, one major theme of neuroscience was a switch from the analysis of individual neurons to recording ever-larger numbers of cells and trying to understand their joint behavior - the population activity. In many ways, this development was unavoidable and the result of a simple numbers game: while some invertebrate species only have a small number of highly stereotyped neurons (in the nematode C. elegans, all 302 neurons are even uniquely named and can be identified across animals), most animals have far over 100 000 neurons. As one enters the class of mammals, that number can safely be assumed to be in the millions.\nIn order to better understand what the concurrent activity in all these cells mean, it is not sufficient to simply increase the number of neurons one records from. In the end, one is left with a gigantic matrix to make sense of and without a way to simplify, that is a rather daunting task . One important insight was that in a population of neurons, the activity of some subpopulations tends to be correlated. Depending on the strength of correlations, dimensionality reduction techniques such as principal component analysis can be applied, allowing for a lower-dimensional representation of the dataset without sacrificing too much variability. Treating neuronal population recordings this way allowed for fascinating discoveries, showcasing how the topology of the subspace carved out by neuronal dynamics is intricately linked to the function of the circuit.\nOf course, there is no free lunch and data processing can only ever destroy information. What is sacrificed in this context is knowledge about individual cells and their responses. Now, one might assume that the trade-off is worth it, but progress in biology is dependent on the combination of insights on different levels of understanding. I recently came across a really interesting paper from the lab of Liset Menendez de la Prida, that is beautifully illustrating this point. The publication is called Cell-type-specific manifold analysis discloses independent geometric transformations in the hippocampal spatial code and is quite a read. Julio Esparza and colleagues try to understand the role of genetically defined subpopulations of neurons in the hippocampal subregion CA1 in the formation of spatial representations."
  },
  {
    "objectID": "blog/20250517/index.html#hippocampal-representation-of-a-linear-track",
    "href": "blog/20250517/index.html#hippocampal-representation-of-a-linear-track",
    "title": "Bridging levels - the importance of cell type identity",
    "section": "Hippocampal representation of a linear track",
    "text": "Hippocampal representation of a linear track\nThe ability of the hippocampal formation to represent the place of an animal within its spatial context has been explored for over 50 years. Most notably, when animals move through an environment, some hippocampal cells will only become active whenever a certain place is visited. These kind of cells became known as place cells and form one of the bedrocks of the neuroscientific field of spatial navigation.\nThe cell type identity of these neurons is almost never explicitly addressed. In early electrophysiological recordings, it was simply not possible to discern what kind of neuron one recorded from, and even when it became feasible, questions along these lines never took center-stage. However, the hippocampus is not a monolithic tissue with only one kind of neuron, but an intricate anatomical structure made up of a diverse set of cell types. This is indeed a general feature of neural circuits that, with the advent of single-cell RNA-sequencing, has become more and more appreciated and to refer to the excitable cells inside the brain simply as neurons doesn`t do justice to the multiplicity of shapes and functions observed and described. In the paper, Esparza et al. ask about the contribution of differently located pyramidal cells (PCs) to the representation of a linear track. To be precise, they are able to perform targeted recordings either from superficial or deep PCs (they do this be using either transgenic animals or deliver calcium reporters to gene populations of interest via virus injections).\nThis approach facilitates a peculiar take on the population level analysis. Normally, one would temporally bin neuronal activity and depict the firing rate (or the corresponding proxy) of each neuron on one axis of an n-dimensional space, with n being the number of recorded neurons. In this space, one can now analyse the features of the subspace being carved out by the population analysis. In this case, however, the authors can additionally ask what kind of properties are contributed by which cell types. For a first characterization, they simply captured hippocampal activity from an animal running back and forth on a linear track. Consistent with what one would expect from previous research, the resulting subspace occupied by neuronal activity (the manifold) is best described as a tridimensional ring for both kind of neuronal subpopulations. However, the representations differed in subtle ways the authors took great care to characterize. As a side note, the methodological considerations put forward were one of the highlights of this publication for me. Every step of the analysis was motivated with great care and described simply and intuitively. I would recommend to check out the github repo to get an impression of what accompanying code should look like. For example, the authors use a method called angle-based intrinsic dimensionality (ABID) as described by Thordsen et al., 2020 to determine the above mentioned tridimensionality of the observed manifold. This method computes the directional vectors for points neighboring a reference point in an n-dimensional space, and computes the average pairwise cosine similarity for these vectors. The intrinsic dimensionality is then given by the inverse of this value. If the data lives in a high-dimensional space, the vectors will point in random directions and the cosine similarity is low, thus resulting in a high ABID value. Pure algorithmic beauty! Anyway, back to the paper."
  },
  {
    "objectID": "blog/20250517/index.html#as-the-environment-changes-so-does-the-representation",
    "href": "blog/20250517/index.html#as-the-environment-changes-so-does-the-representation",
    "title": "Bridging levels - the importance of cell type identity",
    "section": "As the environment changes, so does the representation",
    "text": "As the environment changes, so does the representation\nThe most striking observation about the ring-like manifolds was the difference in their responses to the manipulation of spatial cues. As a simple experimental intervention, the track was simply rotated inside the lab, resulting in a 180 ° mismatch between local and global cues for the rodents navigating the track before and after this rotation. For both superficial and deep neurons, the manipulation resulted in a displacement of the ring. However, the ring-like representation of the deep neurons was additionally rotated by 180 °, matching the change in the outside world. Fascinatingly, for someone simply recording from the entirety of CA1, this manifold rotation would not be visible as this pooling over different cell populations appears to mask some subpopulation-specific features. To me, this is an insanely important insight. These days, neuroscientists push for ever-increasing FOVs, better genetically encoded activity indicators, and deliberately record from tiny animals as these creatures enable measurements from the entire nervous system at once. Indeed, there is value in completeness because somewhere in the resulting dataset, there is the information we are after. However, Esparza and colleagues remind us here of another important concept of science: to carve reality at its joints. To simply record large datasets without knowing how to subdivide, cluster, and analyze them might actually be detrimental. The scientist recording the entire CA1 population might wrongly conclude that the rotational mismatch simply induces a displacement of the manifold, while there are actually subpopulations tracking the precise rotation between local and global cues. The identity of the cells we record from is necessary to stratify the ever-growing datasets and only by doing so will we not be blind to computations otherwise lost in the noise of completeness."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog! Here you’ll find posts on neuroscience, coding, and research life.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCute Pandas and Fast Polars\n\n\n\nworking in science\n\nanalysis\n\nprogramming\n\n\n\n\n\n\n\n\n\nSep 21, 2025\n\n\nLukas Anneser\n\n\n\n\n\n\n\n\n\n\n\n\nFalse friends in biology\n\n\n\nworking in science\n\nHPS\n\n\n\n\n\n\n\n\n\nJul 6, 2025\n\n\nLukas Anneser\n\n\n\n\n\n\n\n\n\n\n\n\nBridging levels - the importance of cell type identity\n\n\n\nneuroscience\n\njournal club\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 8, 2025\n\n\nLukas Anneser\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "CV",
    "section": "",
    "text": "A well-rounded neuroscientist with strong medical, molecular biology, and bioinformatic background. My focus is to understand state-dependent information processing and neuromodulation."
  },
  {
    "objectID": "CV.html#talks",
    "href": "CV.html#talks",
    "title": "CV",
    "section": "Talks",
    "text": "Talks\n\n2024 – EMBO Fellows Meeting, Heidelberg\n\n2024 – iRTG Seminar, Oldenburg\n\n2024 – Basel Neuroscience Workshop, Basel\n\n2023 – Zebrafish Neural Circuits & Behavior Seminar (virtual)\n\n2023 – International Biophysics Workshop, LUMS, Pakistan (virtual)\n\n2021 – Max Planck Society PhDnet Symposium (virtual)\n\n2021 – Neurobiology Department Seminar, University of Konstanz\n\n2021 – Biologicum Department Seminar, Goethe University Frankfurt\n\n2021 – Zebrafish Husbandry Workshop (virtual)\n\n2020 – JRNLclub (virtual)\n\n2019 – Junior Scientist Workshop on Mechanistic Cognitive Neuroscience, Janelia, Ashburn\n\n2019 – Neural Systems and Behavior Course, MBL, Woods Hole"
  },
  {
    "objectID": "CV.html#posters",
    "href": "CV.html#posters",
    "title": "CV",
    "section": "Posters",
    "text": "Posters\n\nOctober 2024 – Anneser L, Satou C, Hotz HR, Caudullo T, Javier A, van Krugten J, Eckhard J, Palacios Flores K, Friedrich R.\nMolecular, functional, and behavioral analysis of neuromodulatory networks in the zebrafish telencephalon.\nEMBO Fellows Meeting, October 25–29, Heidelberg, Germany.\nJune 2024 – Anneser L, Satou C, Hotz HR, Caudullo T, Javier A, van Krugten J, Eckhard J, Palacios Flores K, Friedrich R.\nMolecular, functional, and behavioral analysis of neuromodulatory networks in the zebrafish telencephalon.\nFENS Forum, June 25–29, Vienna, Austria.\nMay 2023 – Anneser L, Satou C, Friedrich R.\nCombinatorial logic of neuromodulatory systems in the zebrafish telencephalon.\nGordon Research Conference on Modulation of Neural Circuits and Behavior, May 20–26, Les Diablerets, Switzerland.\nJuly 2022 – Anneser L, Satou C, Friedrich R.\nCombinatorial logic of neuromodulatory systems in the zebrafish telencephalon.\nInternational Conference on Neuroethology, July 25–28, Lisbon, Portugal.\nNovember 2019 – Anneser L, Alcantara IC, Gemmer A, Soojin R, Schuman EM.\nBrain transcriptional dynamics and social environment information.\nWorkshop on Mechanistic Cognitive Neuroscience, October 27 – November 1, Janelia, Ashburn, USA.\nJanuary 2019 – Anneser L, Alcantara IC, Gemmer A, Soojin R, Schuman EM.\nEffect of social isolation on gene transcription in zebrafish.\nFENS-Hertie Winter School: Neural Control of Instinctive and Innate Behavior, January 6–12, Obergurgl, Austria.\nNovember 2018 – Anneser L, Alcantara IC, Gemmer A, Soojin R, Schuman EM.\nImpact of the social environment on gene expression in zebrafish.\n5th Imaging Structure and Function in the Zebrafish Brain Conference, November 30 – December 2, Brighton, UK."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "publications",
    "section": "",
    "text": "Anneser L*, Kappel JM* (2025). Conserved multisensory integration of social cues in the thalamus.\niScience, 28(1):111678. Review Here, Johannes Kappel and I integrate the work we did in our PhD into a larger context: So far, it is still not satisfyingly answered how social representations emerge in vertebrate brains. It was a curious observation to see that very specific sensory aspects of the social environment such as visual cues and mechanosensation via the lateral line both impinge on the same thalamic area in zebrafish. We review the evidence of dedicated “social” frequencies across different modalities and species that jointly elicit activity in subparafascicular area of the thalamus, thus potentially conveying specific social information.\nAnneser L, Satou C, Hotz HR, Friedrich R (2024). Molecular organization of neuronal cell types and neuromodulatory systems in the zebrafish telencephalon. Current Biology, 34, 1–15. My first postdoc paper! Here, we try and chart the cell type territory of the adult zebrafish telencephalon using single-cell sequencing and gene expression visualization. We give a comprehensive overview of the neuronal cell type repertoire, the localization of specific neuronal subtypes across the pallium and subpallium, identify potential homologies across vertebrates and map out neuromodulatory signatures (i.e., expression of specific GPCRs, neuropeptides and modulation-associated genes).\nGemmer A, Mirkes K, Anneser L, Eilers T, Kibat C, Mathuru A, Ryu S, Schuman EM (2022). Oxytocin receptors influence the development and maintenance of social behavior in zebrafish (Danio rerio). Scientific Reports, 12(4322)."
  }
]